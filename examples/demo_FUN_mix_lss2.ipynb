{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Demo of functions load\\_lss2\\_clean & updown\\_sample from*./swiss_utils/data_cube_utilities/sdc_utilities.py*\n",
    "\n",
    "*****\n",
    "\n",
    "__This script is the \"official demo\" of a function. Please if you want to modify it, work on your own copy__\n",
    "\n",
    "Creates a clean dataset mixing Landsat and Sentinel 2 products (respectively with prefixs 'ls' and 's2') and using cleaning \"autor's recommended ways\":\n",
    "- ls_qa_clean\n",
    "- create_slc_clean_mask\n",
    "\n",
    "Sorted by ascending time.\n",
    "\n",
    "If `resample` option is activated ('up' or 'down_mean', 'down_median') up/downsampling is performed and products output combined into a single 'lss2' prefix. \n",
    "\n",
    "This function works as [demo_FUN_load_multi_clean.ipynb](demo_FUN_load_multi_clean.ipynb) function, but with a mix of Landsat and Sentinel 2 products. The `resampl` option was added (to optionally combine products output).\n",
    "\n",
    "<mark>!!! This function was developped specifically for the first version of SDC (which only used ingested products) then it will have to be rewritten for the actual version of SDC !!!</mark>\n",
    "\n",
    "__load\\_lss2\\_clean__:\n",
    "* dc: datacube.api.core.Datacube, the Datacube instance to load data with.\n",
    "* products: list of products\n",
    "* time: pair (list) of start and end dates (end date is not included (< instead of <=) !)\n",
    "* lon: pair (list) of minimum and maximum longitude\n",
    "* lat: pair (list) of minimum and maximum longitude\n",
    "* measurements: list of measurements (without mask band, landsat and Sentinel 2 products prefix shouls be 'ls or 's2)\n",
    "* resampl: (OPTIONAL) Up/Downsample ('up', 'down_mean', 'down_median' ) products and combine their output\n",
    "* valid_cats:   (OPTIONAL) list of list of ints representing what category should be considered valid first Landsat categories, then Sentinel 2 categories\n",
    "\n",
    "__updown\\_sample__:\n",
    "* ds_l: 'large' resolution xarray.Dataset (so far Landat product)\n",
    "* ds_s: 'small' resolution xarray.Dataset (so far Sentinel 2 product)\n",
    "* resampl: 'up' to upsample, 'down_mean' to downsample using mean values and 'down_median' to downsample using median values\n",
    "\n",
    "Documentation for a given function can be accessed simply by adding ? at the end of the function in a cell. e.g. `load_lss2_clean?` or by selecting the function and pressing `Shift-Tab`.\n",
    "\n",
    "In this demo Jupyter script, the user can either use the in-script function (below) or import it from ./swiss_utils/data_cube_utilities/sdc_utilities.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the script is using the proper kernel\n",
    "try:\n",
    "    %run ../swiss_utils/assert_env.py\n",
    "except:\n",
    "    %run ./swiss_utils/assert_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "# reload module before executing code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# define modules locations (you might have to adapt define_mod_locs.py)\n",
    "%run ../swiss_utils/define_mod_locs.py\n",
    "\n",
    "# to plot figures\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import colors\n",
    "\n",
    "from utils.data_cube_utilities.dc_mosaic import create_hdmedians_multiple_band_mosaic\n",
    "\n",
    "from swiss_utils.data_cube_utilities.sdc_advutils import composite_fig, oneband_fig\n",
    "\n",
    "import datacube\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "# AND THE FUNCTION\n",
    "from swiss_utils.data_cube_utilities.sdc_utilities import load_lss2_clean, updown_sample"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In-script function\n",
    "# DO NOT RUN THIS CELL IF YOU WANT TO USE THE IMPORTED FUNCTION (LAST LINE OF CELL ABOVE)\n",
    "# To make sure to not run inadvertently this cell convert it to Raw NBConvert\n",
    "\n",
    "from swiss_utils.data_cube_utilities.sdc_utilities import load_multi_clean\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "# source: https://stackoverflow.com/questions/32846846/quick-way-to-upsample-numpy-array-by-nearest-neighbor-tiling\n",
    "def tile_array(a, x0, x1, x2):\n",
    "    t, r, c = a.shape                                    # number of rows/columns\n",
    "    ts, rs, cs = a.strides                                # row/column strides \n",
    "    x = as_strided(a, (t, x0, r, x1, c, x2), (ts, 0, rs, 0, cs, 0)) # view a as larger 4D array\n",
    "    return x.reshape(t*x0, r*x1, c*x2)                      # create new 2D array\n",
    "\n",
    "def updown_sample(ds_l, ds_s, resampl):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "      Up or down sample a \"large\" resolution xarray.Dataset (so far Landsat products) and a \"small\" resolution\n",
    "      xarray.Dataset (so far Sentinel 2 product) and combine them into a single xarray.Dataset.\n",
    "      \"large\" resolution must be a multiple of \"small\" resolution and geographical extent must be adequate.\n",
    "      Xarray.Dataset need to be cleaned as mask band will be removed from the output\n",
    "      To enforce this requirement usage of load_lss2_clean function (without the resampl option) is\n",
    "      highly recommended.\n",
    "\n",
    "    Args:\n",
    "      ds_l:         'large' resolution xarray.Dataset\n",
    "      ds_s:         'small' resolution xarray.Dataset\n",
    "      resampl:      'up' to upsample\n",
    "                    'down_mean' to downsample using mean values\n",
    "                    'down_median' to downsample using median values\n",
    "      \n",
    "    Output:\n",
    "      Upsampled and combined dataset and clean_mask sorted by ascending time.\n",
    "    Authors:\n",
    "      Bruno Chatenoux (UNEP/GRID-Geneva, 11.12.2019)\n",
    "    \"\"\"\n",
    "    \n",
    "    # check resampl options\n",
    "    resampl_opts = ['up', 'down_mean', 'down_median']\n",
    "    assert (resampl in resampl_opts) or (resampl == ''), \\\n",
    "           '\\nif used, resample option must be %s' % resampl_opts\n",
    "    \n",
    "    # check ds ratio\n",
    "    ratiox = len(ds_s.longitude.values) / len(ds_l.longitude.values)\n",
    "    ratioy = len(ds_s.latitude.values) / len(ds_l.latitude.values)\n",
    "    assert (ratiox == 3), \\\n",
    "           '\\nthe ratio of the number of columns should be 3 (Landsat/Sentinel 2 only so far) !'\n",
    "    assert (ratioy == 3), \\\n",
    "           '\\nthe ratio of the number of rows should be 3 (Landsat/Seentinel 2 only so far) !'\n",
    "\n",
    "    # check ds resolutions\n",
    "    resx_l = (ds_l.longitude.values.max() - ds_l.longitude.values.min()) / (len(ds_l.longitude.values) - 1)\n",
    "    resy_l = (ds_l.latitude.values.max() - ds_l.latitude.values.min()) / (len(ds_l.latitude.values) - 1)\n",
    "    resx_s = (ds_s.longitude.values.max() - ds_s.longitude.values.min()) / (len(ds_s.longitude.values) - 1)\n",
    "    resy_s = (ds_s.latitude.values.max() - ds_s.latitude.values.min()) / (len(ds_s.latitude.values) - 1)\n",
    "    # in reason of proper float storage issue, compare resolution with a 0.1% accuracy\n",
    "    assert ((abs(resx_s - resx_l / ratiox) / resx_s * 100) < 0.1), \\\n",
    "           '\\nthe column resolution is not a mutiple of %i !' % (ratiox)\n",
    "    assert ((abs(resy_s - resy_l / ratioy) / resy_s * 100) < 0.1), \\\n",
    "           '\\nthe row resolution is not a mutiple of %i !' % (ratioy)\n",
    "    \n",
    "    # check spacing of ds top left pixel center with a 0.1%\n",
    "    assert ((abs(ds_l.longitude.values.min() - ds_s.longitude.values.min()) - resx_s) < resx_s * 0.001), \\\n",
    "           '\\nthe longitudinal extent of both dataset do not overlay properly !' + \\\n",
    "           '\\nuse load_lss2_clean function to fix this issue'\n",
    "    assert ((abs(ds_l.latitude.values.min() - ds_s.latitude.values.min()) - resy_s) < resy_s * 0.001), \\\n",
    "           '\\nthe latitudinal extent of both dataset do not overlay properly !' + \\\n",
    "           '\\nuse load_lss2_clean function to fix this issue'\n",
    "    \n",
    "    # check vars (without mask band as they will no be combined)\n",
    "    vars_l = [ele for ele in sorted(list(ds_l.data_vars)) if ele not in ['pixel_qa', 'slc']]\n",
    "    vars_s = [ele for ele in sorted(list(ds_s.data_vars)) if ele not in ['pixel_qa', 'slc']]\n",
    "    assert (vars_l == vars_s), \\\n",
    "           '\\nmeasurements in dataset are not identical'\n",
    "    \n",
    "    # upsample \"large\" dataset (using temporary array)\n",
    "    for index, var in enumerate(vars_l):\n",
    "        if resampl == 'up':\n",
    "            arr_l = tile_array(ds_l[var].values, 1, int(ratiox), int(ratioy))\n",
    "            da_l = xr.DataArray(arr_l, dims=['time', 'latitude', 'longitude'])\n",
    "            da_l = da_l.assign_coords(time = ds_l.time,\n",
    "                                        latitude = ds_s.latitude,\n",
    "                                        longitude = ds_s.longitude)\n",
    "            # combine s and l\n",
    "            da = xr.concat([ds_s[var], da_l], dim = 'time')\n",
    "        elif resampl[:5] == 'down_':\n",
    "            # source: https://stackoverflow.com/questions/42463172/how-to-perform-max-mean-pooling-on-a-2d-array-using-numpy/42463491#42463491\n",
    "            # 4x faster than skimage way (who has an issue with median function in the case of large stdev !)\n",
    "            t, lat, lon = ds_s[var].values.shape\n",
    "            nlat = lat // ratiox\n",
    "            nlon = lon // ratioy\n",
    "            if resampl == 'down_median':\n",
    "                arr_s = np.nanmedian(ds_s[var].values[:1*t, :int(nlat*ratioy), :int(nlon*ratiox)]. \\\n",
    "                        reshape(1, t, int(nlat), int(ratioy), int(nlon), int(ratiox)), axis=(0, 3, 5))\n",
    "            elif resampl == 'down_mean':\n",
    "                arr_s = np.nanmean(ds_s[var].values[:1*t, :int(nlat*ratioy), :int(nlon*ratiox)]. \\\n",
    "                        reshape(1, t, int(nlat), int(ratioy), int(nlon), int(ratiox)), axis=(0, 3, 5))\n",
    "            da_s = xr.DataArray(arr_s, dims=['time', 'latitude', 'longitude'])\n",
    "            da_s = da_s.assign_coords(time = ds_s.time,\n",
    "                                      latitude = ds_l.latitude,\n",
    "                                      longitude = ds_l.longitude)\n",
    "            # combine l and s\n",
    "            da = xr.concat([ds_l[var], da_s], dim = 'time')\n",
    "        \n",
    "        if index == 0:   \n",
    "            ds = da.to_dataset(name = var)\n",
    "        else:\n",
    "            ds = ds.merge(da.to_dataset(name = var))\n",
    "\n",
    "    # Sort dataset by ascending time\n",
    "    ds = ds.sortby('time')\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def load_lss2_clean(dc, products, time, lon, lat, measurements,\n",
    "                   resampl = '', valid_cats = [[],[]]):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "      Create a clean dataset mixing Landsat and Sentinel 2 products (respectively with prefixs 'ls' and\n",
    "      's2')\n",
    "      and using cleaning \"autor's recommended ways\":\n",
    "      - ls_qa_clean\n",
    "      - create_slc_clean_mask\n",
    "      Sorted by ascending time\n",
    "      If resample option is activated ('up' or 'down_mean', 'down_median') up/downsampling is performed\n",
    "      and products output combined into a single 'lss2' prefix\n",
    "      This function works as load_multi_clean function, but with a mix of Landsat and Sentinel 2 products\n",
    "      the resampl option was added (to optionally combine products output)\n",
    "\n",
    "    Input:\n",
    "      dc:           datacube.api.core.Datacube\n",
    "                    The Datacube instance to load data with.\n",
    "    Args:\n",
    "      products:     list of products\n",
    "      time:         pair (list) of minimum and maximum date\n",
    "      lon:          pair (list) of minimum and maximum longitude\n",
    "      lat:          pair (list) of minimum and maximum longitude\n",
    "      measurements: list of measurements (without mask band, landsat and Sentinel 2 products prefix\n",
    "                    shouls be 'ls or 's2)\n",
    "      resampl:      (OPTIONAL) Up/Downsample ('up', 'down_mean', 'down_median' ) products and combine\n",
    "                    their output\n",
    "      valid_cats:   (OPTIONAL) list of list of ints representing what category should be considered valid\n",
    "                    first Landsat categories, then Sentinel 2 categories\n",
    "                    * meand category by default\n",
    "      # SENTINEL 2 ################################\n",
    "      #   0 - no data                             #\n",
    "      #   1 - saturated or defective              #\n",
    "      #   2 - dark area pixels                    #\n",
    "      #   3 - cloud_shadows                       #\n",
    "      #   4 * vegetation                          #\n",
    "      #   5 * not vegetated                       #\n",
    "      #   6 * water                               #\n",
    "      #   7 * unclassified                        #\n",
    "      #   8 - cloud medium probability            #\n",
    "      #   9 - cloud high probability              #\n",
    "      #  10 - thin cirrus                         #\n",
    "      #  11 * snow                                #\n",
    "      #############################################\n",
    "      # LANDSAT 5, 7 and 8 ########################\n",
    "      #    0 : Fill                               #\n",
    "      #    1 * Clear                              #\n",
    "      #    2 * Water                              #\n",
    "      #    3 : Cloud shadow                       #\n",
    "      #    4 * Snow                               #\n",
    "      #    5 : Cloud                              #\n",
    "      #   10 : Terrain occlusion (Landsat 8 only) #\n",
    "      #############################################\n",
    "    Output:\n",
    "      cleaned dataset and clean_mask sorted by ascending time stored in dictionnaries,\n",
    "      if no up/downsampling is performed dictionnaries contains the two Landsat and Sentinel 2 output\n",
    "      products\n",
    "    Authors:\n",
    "      Bruno Chatenoux (UNEP/GRID-Geneva, 11.12.2019)\n",
    "    \"\"\"\n",
    "    \n",
    "    # intersect measurements with common measurements\n",
    "    measurement_list = dc.list_measurements(with_pandas=False)\n",
    "    for index, product in enumerate(products):\n",
    "        measurements_for_product = filter(lambda x: x['product'] == product, measurement_list)\n",
    "        valid_measurements_name_array = set(map(lambda x: x['name'], measurements_for_product))\n",
    "        if index == 0:\n",
    "            common_measurements = sorted(valid_measurements_name_array)\n",
    "        else:\n",
    "            common_measurements = sorted(set(common_measurements).intersection(valid_measurements_name_array))\n",
    "    measurements = sorted(set(measurements).intersection(common_measurements))\n",
    "    \n",
    "    # dictionary sensor -> mask band (Higher resolution first !)\n",
    "    dict_sensmask = {'ls':'pixel_qa',\n",
    "                     's2': 'slc'}\n",
    "    \n",
    "    resampl_opts = ['up', 'down_mean', 'down_median']\n",
    "    \n",
    "    # check mix Landsat and Sentinel 2\n",
    "    sensors = []\n",
    "    for product in products:\n",
    "        if product[:2] not in sensors:\n",
    "            sensors.append(product[:2])\n",
    "    assert (sorted(set(sensors)) == sorted(set(dict_sensmask.keys()))), \\\n",
    "           '\\nA mix of Landsat and Sentinel 2 products is required !\\nYou should use load_multi_clean function'\n",
    "    \n",
    "    assert (len(valid_cats) == 2), \\\n",
    "           '\\nvalid_cats argument must be a list of list (read the doc for more details)'\n",
    "    \n",
    "    assert (resampl in resampl_opts) or (resampl == ''), \\\n",
    "           '\\nif used, resample option must be %s' % resampl_opts\n",
    "    \n",
    "    dict_dsc = {}\n",
    "    dict_cm = {}\n",
    "    \n",
    "    # Process first Landsat and then Sentinel 2 (based on dict_sensmask order)\n",
    "    for index, sensor in enumerate(dict_sensmask.keys()):\n",
    "        # fix Sentinel 2 geographical extent based on Landsat dataset\n",
    "        if index == 1:\n",
    "            resx = (dsc.longitude.values.max() - dsc.longitude.values.min()) / len(dsc.longitude.values)\n",
    "            resy = (dsc.latitude.values.max() - dsc.latitude.values.min()) / len(dsc.latitude.values)\n",
    "            lon = (dsc.longitude.values.min() - resx / 3, dsc.longitude.values.max() + resx / 3)\n",
    "            lat = (dsc.latitude.values.min() - resy / 3, dsc.latitude.values.max() + resy / 3)\n",
    "        \n",
    "        dsc, cm = load_multi_clean(dc = dc,\n",
    "                                  products = [prod for prod in products if prod[:2] == sensor] ,\n",
    "                                  time = time,\n",
    "                                  lon = lon,\n",
    "                                  lat = lat,\n",
    "                                  measurements = measurements + [dict_sensmask[sensor]], # append mask band\n",
    "                                  valid_cats = valid_cats[index])\n",
    "        dict_dsc[sensor] = dsc\n",
    "        dict_cm[sensor] = cm\n",
    "    \n",
    "    if resampl in resampl_opts :\n",
    "        dsc = updown_sample(dict_dsc['ls'], dict_dsc['s2'], resampl)\n",
    "        dict_dsc = {}\n",
    "        dict_cm = {}\n",
    "        dict_dsc['lss2'] = dsc\n",
    "        dict_cm['lss2'] = ~np.isnan(dsc[measurements[0]].values)\n",
    "    \n",
    "    return dict_dsc, dict_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the majority of demo_FUN_* notebook, the next cell contains the dataset configuration information:\n",
    "- product\n",
    "- geographical extent\n",
    "- time period\n",
    "- bands\n",
    "\n",
    "But as the purpose of `mix_lss2` function is to mix Landsat and Sentinel 2 products, _product_ string, become _product**s**_ tupple.\n",
    "\n",
    "Time period is also an issue as Sentinel 2 acquisition started in June 2015, start time should be more recent than this period.\n",
    "\n",
    "If you are well aware of your ODC dataset and you understodd well what is written above, you can generate a configuration cell. If you aren't, it recommended to proceed as follows:\n",
    "1. run the [config_tool](config_tool.ipynb) using **Sentinel 2** product and the _measurements_ containing at minimum `['blue', 'green', 'red', 'nir']` (as they are required for the current demo).\n",
    "2. load the generated configuration cell in the next cell by running the magic `%load config_cell.txt`\n",
    "3. use the _Select the product_ cell of [config_tool](config_tool.ipynb) again, to identity Landsat 7 and/or 8 product(s),\n",
    "4. in the next cell rename _product_ variable to _product**s**_, use list instead of strings (e.g. in SDC context `products = ['s2_l2a_10m_swiss', 'ls7_ledaps_swiss', 'ls8_lasrc_swiss']`).\n",
    "\n",
    "Comments:\n",
    "- As mask band differ between Landsat and Sentinel 2, mask band do not need to be added in the measurements list. But in the case they are noth or only one added, the `mix_lss2` function will deal with this issue.\n",
    "- As bands differ between Landsat and Sentinel 2, the `mix_lss2`function will only keep common bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load config_cell.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prepare a dataset combining Landsat and Sentinel 2 products by downsampling (decreasing the resolution) of Sentinel 2 into Landsat resolution (computing the median of 9 Sentinel 2 pixels)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dsc, dict_cm = load_lss2_clean(dc = dc,                                    \n",
    "                                    products = products ,\n",
    "                                    time = [start_date, end_date],\n",
    "                                    lon = (min_lon, max_lon),\n",
    "                                    lat = (min_lat, max_lat),\n",
    "                                    measurements = measurements,\n",
    "                                    resampl = 'down_median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__As both products are combined, the output consists in two dictionnaries containing a single dataset (dataset_clean and clean_mask)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the two dictionnaries\n",
    "\n",
    "print(dict_dsc)\n",
    "print(dict_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mosaic = create_hdmedians_multiple_band_mosaic(dict_dsc['lss2'],\n",
    "                                               dict_cm['lss2'],\n",
    "                                               operation='medoid')\n",
    "composite_fig(mosaic,\n",
    "              bands = ['red', 'green', 'blue'],\n",
    "              title = 'Sentinel 2 and Landsat downsampled - median',\n",
    "              scalebar_color = 'white',\n",
    "              v_min = 0,\n",
    "              v_max = 1500,\n",
    "              max_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Repeat the process but this time by upsampling (increasing resolution) Landsat into Sentinel 2 resolution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dsc, dict_cm = load_lss2_clean(dc = dc,                                    \n",
    "                                    products = products ,\n",
    "                                    time = [start_date, end_date],\n",
    "                                    lon = (min_lon, max_lon),\n",
    "                                    lat = (min_lat, max_lat),\n",
    "                                    measurements = measurements,\n",
    "                                    resampl = 'up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create and plot a composite mosaic\n",
    "\n",
    "mosaic = create_hdmedians_multiple_band_mosaic(dict_dsc['lss2'],\n",
    "                                               dict_cm['lss2'],\n",
    "                                               operation='medoid')\n",
    "composite_fig(mosaic,\n",
    "              bands = ['red', 'green', 'blue'],\n",
    "              title = 'Sentinel 2 and Landsat upsampled',\n",
    "              scalebar_color = 'white',\n",
    "              v_min = 0,\n",
    "              v_max = 1500,\n",
    "              max_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If the _`resampl`_ option is not used, the function will output two dictionnaries containing two dataset__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_dsc, dict_cm = load_lss2_clean(dc = dc,                                    \n",
    "                                    products = products ,\n",
    "                                    time = [start_date, end_date],\n",
    "                                    lon = (min_lon, max_lon),\n",
    "                                    lat = (min_lat, max_lat),\n",
    "                                    measurements = measurements)\n",
    "print(dict_dsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and plot a Sentinel 2 composite mosaic\n",
    "\n",
    "mosaic = create_hdmedians_multiple_band_mosaic(dict_dsc['s2'],\n",
    "                                               dict_cm['s2'],\n",
    "                                               operation='medoid')\n",
    "composite_fig(mosaic,\n",
    "              bands = ['red', 'green', 'blue'],\n",
    "              title = 'Sentinel 2 only',\n",
    "              scalebar_color = 'white',\n",
    "              v_min = 0,\n",
    "              v_max = 1500,\n",
    "              max_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__As upscaling increases the size of the large resolution dataset without increasing the level of information of the dataset, it is a waste of resources to derive products from upscaled dataset.__\n",
    "\n",
    "__Let's compute NDVI of ls and s2 dataset and then combine them by upscaling__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute NDVI\n",
    "ndvi_s2 = (dict_dsc['s2'].nir - dict_dsc['s2'].red) / (dict_dsc['s2'].nir + dict_dsc['s2'].red)\n",
    "ndvi_ls = (dict_dsc['ls'].nir - dict_dsc['ls'].red) / (dict_dsc['ls'].nir + dict_dsc['ls'].red)\n",
    "# combine by upscaling\n",
    "# ndvi dataArray need to be converted to dataset\n",
    "ndvi_lss2 = updown_sample(ndvi_ls.to_dataset(name = 'ndvi'),\n",
    "                          ndvi_s2.to_dataset(name = 'ndvi'),\n",
    "                                             resampl = 'up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finally lets compute ls, s2 and upscaled lss2 and visualize NDVI mean__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ls\n",
    "ndvi_ls_mean = ndvi_ls.mean(dim=['time'])\n",
    "oneband_fig(ndvi_ls_mean,\n",
    "            leg = colors.LinearSegmentedColormap.from_list('ndvi', ['darkblue','blue','lightblue',\n",
    "                                                                    'lightgreen','darkgreen'], N=256),\n",
    "            title = 'Landsat only NDVI mean',\n",
    "            scalebar_color = 'white',\n",
    "            max_size = 10,\n",
    "            v_min = -1,\n",
    "            v_max = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2\n",
    "ndvi_s2_mean = ndvi_s2.mean(dim=['time'])\n",
    "oneband_fig(ndvi_s2_mean,\n",
    "            leg = colors.LinearSegmentedColormap.from_list('ndvi', ['darkblue','blue','lightblue',\n",
    "                                                                    'lightgreen','darkgreen'], N=256),\n",
    "            title = 'Sentinel 2 only NDVI mean',\n",
    "            scalebar_color = 'white',\n",
    "            max_size = 10,\n",
    "            v_min = -1,\n",
    "            v_max = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upscaled lss2\n",
    "ndvi_lss2_mean = ndvi_lss2.mean(dim=['time'])\n",
    "oneband_fig(ndvi_lss2_mean.ndvi,\n",
    "            leg = colors.LinearSegmentedColormap.from_list('ndvi', ['darkblue','blue','lightblue',\n",
    "                                                                    'lightgreen','darkgreen'], N=256),\n",
    "            title = 'Sentinel 2 and Landsat upsampled NDVI mean',\n",
    "            scalebar_color = 'white',\n",
    "            max_size = 10,\n",
    "            v_min = -1,\n",
    "            v_max = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Open Data Cube Development)",
   "language": "python",
   "name": "odc-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
