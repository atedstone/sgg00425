{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of functions ds\\_focus from*./swiss_utils/data_cube_utilities/sdc_devtools.py*\n",
    "\n",
    "*****\n",
    "\n",
    "__This script is the \"official demo\" of a function. Please if you want to modify it, work on your own copy__\n",
    "\n",
    "In order to ease the creation of demo dataset to be used during development, the `ds_focus` function selects within an xarray.Dataset (`ds`) the AOI (size given by `dough_width`) with sum of min, max values (defined by `stat`argument). Then focus can be automatically given on a small datset with maximum or minimum of data.\n",
    "\n",
    "__ds_focus__:\n",
    "- **ds**: xarray.Dataset to to be focused on.\n",
    "- **dough_width** : doughnut width (window size will be 2 * dough_size + 1).\n",
    "- **stat** : stat type to apply for window selection.\n",
    "\n",
    "Documentation for a given function can be accessed simply by adding ? at the end of the function in a cell. e.g. `ds_focus?` or by selecting the function and pressing `Shift-Tab`.\n",
    "\n",
    "In this demo Jupyter script, the user can either use the in-script function (below) or import it from ./swiss_utils/data_cube_utilities/sdc_devtools.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the script is using the proper kernel\n",
    "try:\n",
    "    %run ../swiss_utils/assert_env.py\n",
    "except:\n",
    "    %run ./swiss_utils/assert_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "# reload module before executing code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# define modules locations (you might have to adapt define_mod_locs.py)\n",
    "%run ../swiss_utils/define_mod_locs.py\n",
    "\n",
    "# to plot figures\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import colors\n",
    "\n",
    "from swiss_utils.data_cube_utilities.sdc_utilities import load_multi_clean\n",
    "from swiss_utils.data_cube_utilities.sdc_advutils import draw_map, oneband_fig\n",
    "\n",
    "import datacube\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "# AND THE FUNCTION\n",
    "from swiss_utils.data_cube_utilities.sdc_devtools import ds_focus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In-script function\n",
    "# DO NOT RUN THIS CELL IF YOU WANT TO USE THE IMPORTED FUNCTION (LAST LINE OF CELL ABOVE)\n",
    "# To make sure to not run inadvertently this cell convert it to Raw NBConvert\n",
    "\n",
    "# source: https://gist.github.com/seberg/3866040\n",
    "def rolling_window(array, window=(0,), asteps=None, wsteps=None, axes=None, toend=True):\n",
    "    \"\"\"\n",
    "    Create a view of `array` which for every point gives the n-dimensional\n",
    "    neighbourhood of size window. New dimensions are added at the end of\n",
    "    `array` or after the corresponding original dimension.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : array_like\n",
    "        Array to which the rolling window is applied.\n",
    "    window : int or tuple\n",
    "        Either a single integer to create a window of only the last axis or a\n",
    "        tuple to create it for the last len(window) axes. 0 can be used as a\n",
    "        to ignore a dimension in the window.\n",
    "    asteps : tuple\n",
    "        Aligned at the last axis, new steps for the original array, ie. for\n",
    "        creation of non-overlapping windows. (Equivalent to slicing result)\n",
    "    wsteps : int or tuple (same size as window)\n",
    "        steps for the added window dimensions. These can be 0 to repeat values\n",
    "        along the axis.\n",
    "    axes: int or tuple\n",
    "        If given, must have the same size as window. In this case window is\n",
    "        interpreted as the size in the dimension given by axes. IE. a window\n",
    "        of (2, 1) is equivalent to window=2 and axis=-2.       \n",
    "    toend : bool\n",
    "        If False, the new dimensions are right after the corresponding original\n",
    "        dimension, instead of at the end of the array. Adding the new axes at the\n",
    "        end makes it easier to get the neighborhood, however toend=False will give\n",
    "        a more intuitive result if you view the whole array.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A view on `array` which is smaller to fit the windows and has windows added\n",
    "    dimensions (0s not counting), ie. every point of `array` is an array of size\n",
    "    window.\n",
    "    \"\"\"\n",
    "    \n",
    "    array = np.asarray(array)\n",
    "    orig_shape = np.asarray(array.shape)\n",
    "    window = np.atleast_1d(window).astype(int) # maybe crude to cast to int...\n",
    "    \n",
    "    if axes is not None:\n",
    "        axes = np.atleast_1d(axes)\n",
    "        w = np.zeros(array.ndim, dtype=int)\n",
    "        for axis, size in zip(axes, window):\n",
    "            w[axis] = size\n",
    "        window = w\n",
    "    \n",
    "    # Check if window is legal:\n",
    "    if window.ndim > 1:\n",
    "        raise ValueError(\"`window` must be one-dimensional.\")\n",
    "    if np.any(window < 0):\n",
    "        raise ValueError(\"All elements of `window` must be larger then 1.\")\n",
    "    if len(array.shape) < len(window):\n",
    "        raise ValueError(\"`window` length must be less or equal `array` dimension.\") \n",
    "\n",
    "    _asteps = np.ones_like(orig_shape)\n",
    "    if asteps is not None:\n",
    "        asteps = np.atleast_1d(asteps)\n",
    "        if asteps.ndim != 1:\n",
    "            raise ValueError(\"`asteps` must be either a scalar or one dimensional.\")\n",
    "        if len(asteps) > array.ndim:\n",
    "            raise ValueError(\"`asteps` cannot be longer then the `array` dimension.\")\n",
    "        # does not enforce alignment, so that steps can be same as window too.\n",
    "        _asteps[-len(asteps):] = asteps\n",
    "        \n",
    "        if np.any(asteps < 1):\n",
    "             raise ValueError(\"All elements of `asteps` must be larger then 1.\")\n",
    "    asteps = _asteps\n",
    "    \n",
    "    _wsteps = np.ones_like(window)\n",
    "    if wsteps is not None:\n",
    "        wsteps = np.atleast_1d(wsteps)\n",
    "        if wsteps.shape != window.shape:\n",
    "            raise ValueError(\"`wsteps` must have the same shape as `window`.\")\n",
    "        if np.any(wsteps < 0):\n",
    "             raise ValueError(\"All elements of `wsteps` must be larger then 0.\")\n",
    "\n",
    "        _wsteps[:] = wsteps\n",
    "        _wsteps[window == 0] = 1 # make sure that steps are 1 for non-existing dims.\n",
    "    wsteps = _wsteps\n",
    "\n",
    "    # Check that the window would not be larger then the original:\n",
    "    if np.any(orig_shape[-len(window):] < window * wsteps):\n",
    "        raise ValueError(\"`window` * `wsteps` larger then `array` in at least one dimension.\")\n",
    "\n",
    "    new_shape = orig_shape # just renaming...\n",
    "    \n",
    "    # For calculating the new shape 0s must act like 1s:\n",
    "    _window = window.copy()\n",
    "    _window[_window==0] = 1\n",
    "    \n",
    "    new_shape[-len(window):] += wsteps - _window * wsteps\n",
    "    new_shape = (new_shape + asteps - 1) // asteps\n",
    "    # make sure the new_shape is at least 1 in any \"old\" dimension (ie. steps\n",
    "    # is (too) large, but we do not care.\n",
    "    new_shape[new_shape < 1] = 1\n",
    "    shape = new_shape\n",
    "    \n",
    "    strides = np.asarray(array.strides)\n",
    "    strides *= asteps\n",
    "    new_strides = array.strides[-len(window):] * wsteps\n",
    "    \n",
    "    # The full new shape and strides:\n",
    "    if toend:\n",
    "        new_shape = np.concatenate((shape, window))\n",
    "        new_strides = np.concatenate((strides, new_strides))\n",
    "    else:\n",
    "        _ = np.zeros_like(shape)\n",
    "        _[-len(window):] = window\n",
    "        _window = _.copy()\n",
    "        _[-len(window):] = new_strides\n",
    "        _new_strides = _\n",
    "        \n",
    "        new_shape = np.zeros(len(shape)*2, dtype=int)\n",
    "        new_strides = np.zeros(len(shape)*2, dtype=int)\n",
    "        \n",
    "        new_shape[::2] = shape\n",
    "        new_strides[::2] = strides\n",
    "        new_shape[1::2] = _window\n",
    "        new_strides[1::2] = _new_strides\n",
    "    \n",
    "    new_strides = new_strides[new_shape != 0]\n",
    "    new_shape = new_shape[new_shape != 0]\n",
    "    \n",
    "    return np.lib.stride_tricks.as_strided(array, shape=new_shape, strides=new_strides)\n",
    "\n",
    "def ds_focus(ds, dough_width, stat):\n",
    "    \"\"\"\n",
    "    Select within an `array` the window (size given by `dough_width`) with sum of min, max values\n",
    "    (defined by `stat`argument).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray.Dataset\n",
    "        Xarray.Dataset to to be focused on.\n",
    "    dough_width : int\n",
    "        doughnut width (window size will be 2 * dough_size + 1).\n",
    "    stat : string ('min', 'max')\n",
    "        stat type to apply for window selection.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An extract of ds containing the min or max sum of data count.\n",
    "    \"\"\"\n",
    "    # check ds\n",
    "    assert ('Dataset' in str(type(ds))), \\\n",
    "           '\\n<ds> must be an xarray.Dataset !'\n",
    "    \n",
    "    # check dough_width\n",
    "    assert ((isinstance(dough_width, int)) & (dough_width > 0)), \\\n",
    "           '\\n<dough_width> must be a positive integer !'\n",
    "    assert (min(len(ds.latitude), len(ds.longitude)) > dough_width * 2 + 1), \\\n",
    "           '\\n<dough_width> should make a window smaller than the dataset !'\n",
    "    \n",
    "    # check start arguments\n",
    "    stat_args = ['min', 'max']\n",
    "    assert (stat in stat_args), \\\n",
    "           '\\n<stat> argument must be one element of the list %s !' % stat_args\n",
    "    \n",
    "    # data count through time for the first band\n",
    "    arr = ds[list(ds.var())[0]].count(dim=['time']).values\n",
    "    \n",
    "    # apply a \"rolling window\" sum filter on the count\n",
    "    sums = rolling_window(arr, (dough_width * 2 + 1, dough_width * 2 + 1)).sum((2,3))\n",
    "    \n",
    "    # get the coords of the first pixel with targimum sum value\n",
    "    sums_da = xr.DataArray(sums, dims = ['latitude', 'longitude'])\n",
    "    sums_da = sums_da.assign_coords(latitude = ds.latitude[dough_width:len(ds.latitude) - dough_width],\n",
    "                                    longitude = ds.longitude[dough_width:len(ds.longitude) - dough_width])\n",
    "    if stat == 'min':\n",
    "        targ_sum = sums_da.where(sums_da == sums_da.values.min(), drop=True)\n",
    "    elif stat == 'max':\n",
    "        targ_sum = sums_da.where(sums_da == sums_da.values.max(), drop=True)          \n",
    "    targ_sum = targ_sum.to_dataframe(name = 'count').dropna().reset_index()[:1]\n",
    "    \n",
    "    # get new AOI\n",
    "    ctr_lat_index = np.where(ds.latitude.values == float(targ_sum['latitude']))[0]\n",
    "    ctr_lon_index = np.where(ds.longitude.values == float(targ_sum['longitude']))[0]\n",
    "    # index seems inverted for latitude !!!\n",
    "    targ_min_lat = ds.latitude.values[ctr_lat_index + dough_width][0]\n",
    "    targ_max_lat = ds.latitude.values[ctr_lat_index - dough_width][0]\n",
    "    targ_min_lon = ds.longitude.values[ctr_lon_index - dough_width][0]\n",
    "    targ_max_lon = ds.longitude.values[ctr_lon_index + dough_width][0]\n",
    "\n",
    "    # subset dataset (and mask)\n",
    "    targ_ds = ds.isel(latitude = (ds.latitude >= targ_min_lat) &\n",
    "                                 (ds.latitude <= targ_max_lat),\n",
    "                     longitude = (ds.longitude >= targ_min_lon) &\n",
    "                                 (ds.longitude <= targ_max_lon))\n",
    "    \n",
    "    return targ_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First create a large, possibly with dataset with Landsat 7 as this product is know to contains plenty of nodata since 2003."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the dataset configuration information:\n",
    "- product\n",
    "- geographical extent\n",
    "- time period\n",
    "- bands\n",
    "\n",
    "You can generate it in three ways:\n",
    "1. manually from scratch,\n",
    "2. by manually copy/pasting the final cell content of the [config_tool](config_tool.ipynb) notebook,\n",
    "3. by loading the final cell content of the [config_tool](config_tool.ipynb) notebook using the magic `# %load config_cell.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load config_cell.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_map([min_lat, max_lat], [min_lon, max_lon], draw = False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and clean it\n",
    "dataset_clean, clean_mask = load_multi_clean(dc = dc,\n",
    "                                             products = product ,\n",
    "                                             time = [start_date, end_date],\n",
    "                                             lon = (min_lon, max_lon),\n",
    "                                             lat = (min_lat, max_lat),\n",
    "                                             measurements = measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As can be seen in the next cell the number of data through time differ greatly between pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_count = dataset_clean[measurements[0]].count(dim=['time'])\n",
    "oneband_fig(dataset_count,\n",
    "            leg = colors.LinearSegmentedColormap.from_list('redtogreen', ['darkred', 'darkgreen'], N=256),\n",
    "            title = 'Data count',\n",
    "            scalebar_color = 'white',\n",
    "            max_size = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's pretend, as a developer you are interest to start writing your code with location containing as much data as possible (green pixels in the figure above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dough_width (dough for doughnut) value of 25 will create an AOI of 51 x 51 pixels (2 * 25 + 1)\n",
    "ds_dev = ds_focus(ds = dataset_clean, dough_width = 25, stat = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the development dataset\n",
    "\n",
    "dataset_count = ds_dev[measurements[0]].count(dim=['time'])\n",
    "oneband_fig(dataset_count,\n",
    "            leg = colors.LinearSegmentedColormap.from_list('redtogreen', ['darkred', 'darkgreen'], N=256),\n",
    "            title = 'Data count',\n",
    "            scalebar_color = 'white',\n",
    "            max_size = 6)\n",
    "\n",
    "draw_map(lat_ext = (ds_dev.latitude.values.min(), ds_dev.latitude.values.max()),\n",
    "         lon_ext = (ds_dev.longitude.values.min(), ds_dev.longitude.values.max()),\n",
    "         draw = False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once main code written, lets focus on location containing as few data as possible (red pixels in the figure above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply change the stat argument to 'min'\n",
    "ds_dev = ds_focus(ds = dataset_clean, dough_width = 25, stat = 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the new development dataset\n",
    "\n",
    "dataset_count = ds_dev[measurements[0]].count(dim=['time'])\n",
    "oneband_fig(dataset_count,\n",
    "            leg = colors.LinearSegmentedColormap.from_list('redtogreen', ['darkred', 'darkgreen'], N=256),\n",
    "            title = 'Data count',\n",
    "            scalebar_color = 'white',\n",
    "            max_size = 6)\n",
    "\n",
    "draw_map(lat_ext = (ds_dev.latitude.values.min(), ds_dev.latitude.values.max()),\n",
    "         lon_ext = (ds_dev.longitude.values.min(), ds_dev.longitude.values.max()),\n",
    "         draw = False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Open Data Cube Development)",
   "language": "python",
   "name": "odc-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
