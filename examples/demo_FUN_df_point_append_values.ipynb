{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Demo of default functions df\\_point\\_append\\_values\n",
    "\n",
    "*****\n",
    "\n",
    "__This script is the \"official demo\" of a function. Please if you want to modify it, work on your own copy__\n",
    "\n",
    "\n",
    "The function takes a csv dataframe, a xarray.Dataset, the names of the longitude and latitude columns in the csv dataframe, the crs of the csv dataframe, and the shift value (in degrees) to be added to the csv dataframe longitude and latitude columns (in csv dataframe coordinates units)\n",
    "\n",
    "**df**: the dataframe to which you want to append the xarray values  \n",
    "**df_lon**: the name of the longitude column in your csv file  \n",
    "**df_lat**: the name of the latitude column in your csv file  \n",
    "**df_crs**: the coordinate reference system of the csv file  \n",
    "**ds**: the xarray dataset to get values from  \n",
    "**pts_shift**: the number of pixels to shift the csv points up and right , defaults to 0 (optional,\n",
    "this option is usefull when for example points location correspond to the exact corner of the xarray\n",
    "dataset (positive value are appropriate to lower-left corner)\n",
    "\n",
    "\n",
    "Return a dataframe with the same number of rows as the input dataframe, and one column for each of\n",
    "the variables in the xarray.Dataset.\n",
    "\n",
    "Documentation for a given function can be accessed simply by adding ? at the end of the function in a cell. e.g. `df_point_append_values?` or by selecting the function and pressing `Shift-Tab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the script is using the proper kernel\n",
    "try:\n",
    "    %run ../swiss_utils/assert_env.py\n",
    "except:\n",
    "    %run ./swiss_utils/assert_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "# reload module before executing code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# define modules locations (you might have to adapt define_mod_locs.py)\n",
    "%run ../swiss_utils/define_mod_locs.py\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from datetime import datetime\n",
    "from pyproj import Transformer\n",
    "from itertools import product as iterprod\n",
    "\n",
    "from swiss_utils.data_cube_utilities.sdc_utilities import load_multi_clean, write_geotiff_from_xr\n",
    "\n",
    "import datacube\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# AND THE FUNCTION\n",
    "from swiss_utils.data_cube_utilities.sdc_utilities import optimize_types, df_point_append_values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In-script function\n",
    "# DO NOT RUN THIS CELL IF YOU WANT TO USE THE IMPORTED FUNCTION (LAST LINE OF CELL ABOVE)\n",
    "# To make sure to not run inadvertently this cell convert it to Raw NBConvert\n",
    "\n",
    "# source: https://stackoverflow.com/questions/57856010/automatically-optimizing-pandas-dtypes\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "def optimize_types(dataframe):\n",
    "    \"\"\"\n",
    "    The function takes in a dataframe and returns the same dataframe with optimized data types\n",
    "    \n",
    "    :param dataframe: the dataframe to optimize\n",
    "    :return: the dataframe with the optimized types.\n",
    "    \"\"\"\n",
    "    np_types = [np.int8 ,np.int16 ,np.int32, np.int64,\n",
    "               np.uint8 ,np.uint16, np.uint32, np.uint64]\n",
    "    np_types = [np_type.__name__ for np_type in np_types]\n",
    "    type_df = pd.DataFrame(data=np_types, columns=['class_type'])\n",
    "\n",
    "    type_df['min_value'] = type_df['class_type'].apply(lambda row: np.iinfo(row).min)\n",
    "    type_df['max_value'] = type_df['class_type'].apply(lambda row: np.iinfo(row).max)\n",
    "    type_df['range'] = type_df['max_value'] - type_df['min_value']\n",
    "    type_df.sort_values(by='range', inplace=True)\n",
    "    for col in dataframe.loc[:, dataframe.dtypes <= np.integer]:\n",
    "        col_min = dataframe[col].min()\n",
    "        col_max = dataframe[col].max()\n",
    "        temp = type_df[(type_df['min_value'] <= col_min) & (type_df['max_value'] >= col_max)]\n",
    "        optimized_class = temp.loc[temp['range'].idxmin(), 'class_type']\n",
    "        dataframe[col] = dataframe[col].astype(optimized_class)\n",
    "    return dataframe\n",
    "\n",
    "# from pyproj import Transformer\n",
    "# from itertools import product as iterprod\n",
    "def df_point_append_values(df, df_lon, df_lat, df_crs, ds, pts_shift = 0):\n",
    "    \"\"\"\n",
    "    The function takes a csv dataframe, a xarray.Dataset, the names of the longitude and latitude\n",
    "    columns in the csv dataframe, the crs of the csv dataframe, and the shift value (in degrees) to be\n",
    "    added to the csv dataframe longitude and latitude columns (in csv dataframe coordinates units)\n",
    "    \n",
    "    :param df: the dataframe to which you want to append the xarray values\n",
    "    :param df_lon: the name of the longitude column in your csv file\n",
    "    :param df_lat: the name of the latitude column in your csv file\n",
    "    :param df_crs: the coordinate reference system of the csv file\n",
    "    :param ds: the xarray dataset to get values from\n",
    "    :param pts_shift: the number of pixels to shift the csv points up and right , defaults to 0 (optional,\n",
    "    this option is usefull when for example points location correspond to the exact corner of the xarray\n",
    "    dataset (positive value are appropriate to lower-left corner)\n",
    "    :return: A dataframe with the same number of rows as the input dataframe, and one column for each of\n",
    "    the variables in the xarray.Dataset.\n",
    "    \"\"\"\n",
    "    # get the real bbox of the dataset (as by default extent is given to the center of corner pixels)\n",
    "    ds_min_lon = float(ds.longitude.min())\n",
    "    ds_max_lon = float(ds.longitude.max())\n",
    "    ds_min_lat = float(ds.latitude.min())\n",
    "    ds_max_lat = float(ds.latitude.max())\n",
    "\n",
    "    # get the resolution of a pixel\n",
    "    resx = (ds_max_lon - ds_min_lon) / (len(ds.longitude.values) - 1)\n",
    "    resy = (ds_max_lat - ds_min_lat) / (len(ds.latitude.values) - 1)\n",
    "    # extend by half a pixel\n",
    "    ds_min_lon = ds_min_lon - (resx / 2)\n",
    "    ds_max_lon = ds_max_lon + (resx / 2)\n",
    "    ds_min_lat = ds_min_lat - (resy / 2)\n",
    "    ds_max_lat = ds_max_lat + (resy / 2)\n",
    "    \n",
    "    ds_crs = int(ds.crs.split(':')[1])\n",
    "\n",
    "    # reproject real bbox corners from ds to csv CRS\n",
    "    # source: https://hatarilabs.com/ih-en/how-to-translate-coordinate-systems-for-xy-point-data-tables-with-python-pandas-and-pyproj\n",
    "    transformer = Transformer.from_crs(f\"epsg:{ds_crs}\", f\"epsg:{df_crs}\",always_xy=True)\n",
    "    corners = list(iterprod([ds_min_lon, ds_max_lon], [ds_min_lat, ds_max_lat]))\n",
    "    trans_corners = np.array(list(transformer.itransform(corners)))   \n",
    "    \n",
    "    # clip the csv dataframe with reprojected bbox\n",
    "    df = df[(df[df_lon] + pts_shift >= np.min(trans_corners[:, 0])) &\n",
    "            (df[df_lon] + pts_shift <= np.max(trans_corners[:, 0])) &\n",
    "            (df[df_lat] + pts_shift>= np.min(trans_corners[:, 1])) &\n",
    "            (df[df_lat] + pts_shift <= np.max(trans_corners[:, 1]))]\n",
    "    \n",
    "    # reproject csv dataframe coordinates to ds CRS\n",
    "    transformer = Transformer.from_crs(f\"epsg:{df_crs}\", f\"epsg:{ds_crs}\",always_xy=True)\n",
    "    points = list(zip(df[df_lon],df[df_lat]))\n",
    "    trans_coords = np.array(list(transformer.itransform(points)))\n",
    "    \n",
    "    # append trans_coords as get_x and get_y (coordinated to be used to get pixel values in xarray.Dataset)\n",
    "    pd.options.mode.chained_assignment = None # fix for \"A value is trying to be set on a copy of a slice from a DataFrame.\"\n",
    "    df['get_x'] = trans_coords[:,0]\n",
    "    df['get_y'] = trans_coords[:,1]\n",
    "    \n",
    "    # Get values of xarray.Dataset on points coordinates and append to csv dataframe\n",
    "    # get\n",
    "    ds_pts = ds.sel(longitude = xr.DataArray(df.get_x, dims=[\"point\"]),\n",
    "                    latitude = xr.DataArray(df.get_y, dims=[\"point\"]),\n",
    "                    method=\"nearest\")\n",
    "    df_pts = ds_pts.to_dataframe().drop(['latitude', 'longitude'], axis = 1)\n",
    "    if 'time' in df_pts.columns:\n",
    "        df_pts = df_pts.drop(['time'], axis = 1)\n",
    "        \n",
    "    # deal with duplicated column names\n",
    "    for c in list(df_pts.columns):\n",
    "        if c in list(df.columns):\n",
    "            df_pts.rename(columns={c: f\"{c}_joined\"}, inplace=True)\n",
    "        \n",
    "    # append\n",
    "    df = df.drop(['get_x', 'get_y'], axis = 1)\n",
    "    df = df.join(df_pts)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Landsat or Sentinel 2 product, compute NDVI TS statistics and get values\n",
    "\n",
    "The next cell contains the dataset configuration information:\n",
    "- product **should be Landsat or Sentinel 2**\n",
    "- geographical extent\n",
    "- time period\n",
    "- bands **should contains at least ['red', 'nir'] plus the quality band**\n",
    "\n",
    "You can generate it in three ways:\n",
    "1. manually from scratch,\n",
    "2. by manually copy/pasting the final cell content of the [config_tool](config_tool.ipynb) notebook,\n",
    "3. by loading the final cell content of the [config_tool](config_tool.ipynb) notebook using the magic `%load config_cell.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load config_cell.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and clean the dataset\n",
    "\n",
    "ds_clean, clean_mask = load_multi_clean(dc = dc, products = product, time = [start_date, end_date],\n",
    "                                        lon = [min_lon, max_lon], lat = [min_lat, max_lat],\n",
    "                                        measurements = measurements)\n",
    "crs = ds_clean.crs\n",
    "del clean_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute NDVI Time Serie statistics\n",
    "\n",
    "ndvi = (ds_clean.nir - ds_clean.red) / (ds_clean.nir + ds_clean.red)\n",
    "del ds_clean\n",
    "ndvi = ndvi.where(np.isfinite(ndvi)) # replace +-Inf by nan\n",
    "ds_stats = ndvi.mean(dim=['time'], skipna = True).to_dataset(name = 'ndvi_mean')\n",
    "ds_stats = ds_stats.merge(ndvi.median(dim=['time'], skipna = True).to_dataset(name = 'ndvi_median'))\n",
    "ds_stats = ds_stats.merge(ndvi.std(dim=['time'], skipna = True).to_dataset(name = 'ndvi_std'))\n",
    "del ndvi\n",
    "ds_stats.attrs['crs'] = crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a virtual point grid (in EPSG:4326) within the dasat extent\n",
    "\n",
    "In a real situation you can load point file, e.g.:\n",
    "```Python\n",
    "df_csv = pd.read_csv(csv_path, sep = ',')\n",
    "df_csv = optimize_types(df_csv)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a virtual point grid (in EPSG:4326)if you were note able to load a csv point\n",
    "\n",
    "n_rows, n_cols = 50, 50\n",
    "coords = list(iterprod(np.arange(min_lon, max_lon, (max_lon - min_lon) / (n_cols + 1))[1:],\n",
    "                       np.arange(min_lat, max_lat, (max_lat - min_lat) / (n_rows + 1))[1:]))\n",
    "df_csv = pd.DataFrame (coords, columns = ['lon', 'lat'])\n",
    "csv_lon_var = 'lon'\n",
    "csv_lat_var = 'lat'\n",
    "csv_crs = 4326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get values from ds_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = df_point_append_values(df_csv, csv_lon_var, csv_lat_var, csv_crs, ds_stats)\n",
    "print(df_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export output for check values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.to_csv('df_csv.csv')\n",
    "\n",
    "write_geotiff_from_xr(tif_path = 'ds_stats.tif',\n",
    "                      dataset = ds_stats,\n",
    "                      compr = 'DEFLATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Open Data Cube Development)",
   "language": "python",
   "name": "odc-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
