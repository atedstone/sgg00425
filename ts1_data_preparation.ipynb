{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS-1: Data preparation\n",
    "\n",
    "*****\n",
    "\n",
    "This notebook allows you to load and pre-process an SDC dataset, which you can then save into a NetCDF (.nc) file to be reused quickly in other Notebooks where you do your analysis.\n",
    "\n",
    "Things you should change:\n",
    "\n",
    "* The config_cell variables\n",
    "* The output filename of the netcdf file (see the last cell).\n",
    "\n",
    "Then, note that the Notebook has two different options depending on the dataset that you want to pre-process:\n",
    "\n",
    "* Landsat\n",
    "* Land use statistics\n",
    "\n",
    "Only execute the section which corresponds to the product that you specified in the config_cell!\n",
    "\n",
    "*****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "# reload module before executing code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# define modules locations (you might have to adapt define_mod_locs.py)\n",
    "%run ../sdc-notebooks/Tools/define_mod_locs.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "    \n",
    "from datetime import datetime\n",
    "\n",
    "from sdc_tools.sdc_utilities import lsc2_loadcleanscale\n",
    "\n",
    "import datacube\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "ds_clean = None\n",
    "ds_astat = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains the dataset configuration information:\n",
    "- product\n",
    "- geographical extent\n",
    "- time period\n",
    "- bands\n",
    "\n",
    "You can generate it in three ways:\n",
    "1. manually from scratch,\n",
    "2. by manually copy/pasting the final cell content of the [config_tool](config_tool.ipynb) notebook,\n",
    "3. by loading the final cell content of the [config_tool](config_tool.ipynb) notebook using the magic `%load config_cell.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load \"config_cell.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Landsat satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you like, you can load a longer time series of Landsat by requesting data from each satellite.\n",
    "# Be aware that this will take quite a long time to load. \n",
    "# And only do this for an area a few kilometres/10s kilometres in extent (otherwise you risk requesting too much data!)\n",
    "#products = ['landsat_ot_c2_l2', 'landsat_etm_c2_l2', 'landsat_tm_c2_l2']\n",
    "\n",
    "# Be aware that this command cleans the data up, in particular it removes cloudy pixels.\n",
    "# If you want to do your own data cleaning then you need to use dc.load(). See glacier_mapping.ipynb for more information\n",
    "ds_clean, mask = lsc2_loadcleanscale(dc = dc,\n",
    "                                     products = product,\n",
    "                                     longitude = longitude,\n",
    "                                     latitude = latitude,\n",
    "                                     crs = crs,\n",
    "                                     time = time,\n",
    "                                     measurements = measurements,\n",
    "                                     output_crs = output_crs,\n",
    "                                     resolution = resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clean = ds_clean.where(ds_clean >= 0) # keep only positive values\n",
    "ds_clean = ds_clean.dropna('time', how='all') # drop scenes without data\n",
    "ds_clean.time.attrs = {}\n",
    "\n",
    "# Add the requested latitude-longitudes to the metadata just in case they are needed later.\n",
    "ds_clean.attrs['query_longitude'] = longitude\n",
    "ds_clean.attrs['query_latitude'] = latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some necessary small changes so that we can save this dataset to a NetCDF (.nc) file.\n",
    "\n",
    "# Remove quality info attributes\n",
    "if 'pixel_qa' in measurements:\n",
    "    ds_clean.pixel_qa.attrs['flags_definition'] = []\n",
    "elif 'slc' in measurements:\n",
    "    ds_clean.slc.attrs['flags_definition'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: add normalised difference index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL CELL TO CALCULATE NDIs\n",
    "# You can already calculate normalised difference indexes here to be saved with the measurements.\n",
    "# To do this, use the relevant line(s) below and/or add your own.\n",
    "\n",
    "ds_clean['ndvi'] = (ds_clean.nir - ds_clean.red) / (ds_clean.nir + ds_clean.red)\n",
    "ds_clean['ndwi'] = (ds_clean.green - ds_clean.nir) / (ds_clean.green + ds_clean.nir)\n",
    "\n",
    "# Remove time attributes from each of the indices that you define above.\n",
    "ds_clean.ndvi.time.attrs = {}\n",
    "\n",
    "# 'NDWI': '(ds.green - ds.nir) / (ds.green + ds.nir)',\n",
    "# 'NDBI': '(ds.swir2 - ds.nir) / (ds.swir2 + ds.nir)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a quick look at the summary of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add land use statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we manually change the variables `product` and `measurements` to specify what we want to load from arealstatistik.\n",
    "# We leave longitude, latitude, resolution, output_crs exactly as they were for Landsat. \n",
    "# This ensures that the data from arealstatistik will match the spatial coordinates of Landsat perfectly.\n",
    "\n",
    "# Specify the arealstatistik product\n",
    "product = ['arealstatistik']\n",
    "\n",
    "# Here, the measurements are not individual colour bands, \n",
    "# but instead are the different surveys with the desired number of classes.\n",
    "# By default we are loading the surveys for the most recent time period: 2013-2018.\n",
    "# To see all the available surveys, refer to the arealstatistik PDF document and explore_datacube.ipynb.\n",
    "measurements = ['AS18_4', 'AS18_17', 'AS18_27', 'AS18_72']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time is not relevant for the arealstatistik products, so we don't include it as a keyword here.\n",
    "ds_astat = dc.load(product = product,\n",
    "                measurements = measurements,\n",
    "                longitude = longitude,\n",
    "                latitude = latitude,\n",
    "                output_crs = output_crs, \n",
    "                resolution = resolution)\n",
    "# Squeeze to remove the defunct time dimension [otherwise we retain a default timestamp of 1970-01-01, which is not helpful].\n",
    "ds_astat = ds_astat.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a quick look at the summary of these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_astat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, figure out if we need to combine Landsat data with arealstatistik.\n",
    "\n",
    "if (ds_clean is not None) and (ds_astat is not None):\n",
    "    # In this case, you have loaded both Landsat and arealstatistik.\n",
    "    # So, let's combine them into a single Dataset, allowing them to be saved together.\n",
    "    ds_save = xr.merge([ds_clean, ds_astat])\n",
    "elif (ds_clean is not None):\n",
    "    # We are saving only the Landsat dataset\n",
    "    ds_save = ds_clean\n",
    "elif (ds_astat is not None):\n",
    "    # We are saving only the arealstatistik dataset\n",
    "    ds_save = ds_astat\n",
    "else:\n",
    "    raise ValueError('Hmm, unknown combination of data. Ask a teacher for help.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is what will be saved..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file. Change the output filename to something useful!\n",
    "output_filename = 'mydata.nc'\n",
    "ds_save.to_netcdf(output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
