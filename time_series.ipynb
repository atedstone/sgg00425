{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Geospatial time series\n",
    "\n",
    "*****\n",
    "\n",
    "This notebook shows some ideas about time series analysis in a geospatial context. \n",
    "\n",
    "In <a href=\"#part1\">**Part 1**</a>, we look at:\n",
    "- what the temporal dimension is\n",
    "- how you can access the data over this temporal dimension\n",
    "- account for spatial variability\n",
    "- mitigate missing data\n",
    "\n",
    "\n",
    "In <a href=\"#part2\">**Part 2**</a>, we introduce temporal statistics you can derive and how you calculate them:\n",
    "- trends\n",
    "- variability\n",
    "- deviation from the mean\n",
    "<!-- - seasonality -->\n",
    "\n",
    "\n",
    "<!-- In the **3. step** you will learn about some built-in and self-made methods that aim to investigate in more detail the points of step 2, including:\n",
    "- moving windows\n",
    "- statistical tests (a small selection of tests that aim to show different temporal aspects) -->\n",
    "\n",
    "\n",
    "In <a href=\"#part1\">**Part 3**</a>, the ideas from parts 1 and 2 are extended to:\n",
    "- compare different pixels (different time series)\n",
    "\n",
    "By the end, you will be able to use the information stored in all 3 dimensions (x and y - spatial, and z - time). Often these results can be presented again in the form of **maps** that will be covered in a different notebook. \n",
    "*****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    .dothis{\n",
       "    font-weight: bold;\n",
       "    color: #ff7f0e;\n",
       "    font-size:large\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    .dothis{\n",
    "    font-weight: bold;\n",
    "    color: #ff7f0e;\n",
    "    font-size:large\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the script is using the proper kernel\n",
    "try:\n",
    "    %run ../swiss_utils/assert_env.py\n",
    "except:\n",
    "    %run ./swiss_utils/assert_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "\n",
    "# reload module before executing code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# define modules locations (you might have to adapt define_mod_locs.py)\n",
    "%run swiss_utils/define_mod_locs.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.patches import Polygon, Rectangle\n",
    "\n",
    "# from swiss_utils.data_cube_utilities.sdc_utilities import load_multi_clean\n",
    "\n",
    "# import datacube\n",
    "# dc = datacube.Datacube()\n",
    "\n",
    "# silence warning (not recommended during development)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# AND THE FUNCTION\n",
    "# from swiss_utils.data_cube_utilities.sdc_utilities import indices_ts_stats\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16,8)       # this line changes the size of the figures displayed in the notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:8px solid black\" />\n",
    "\n",
    "# Part 0: Preparing/downloading our data\n",
    "\n",
    "We will use a pre-prepared small data subset around Fribourg which we extracted from the Swiss Data Cube for you earlier. <span class=\"dothis\">Download this dataset by running the next cell.</span> After a short while you should see the .nc file appear in the file explorer pane on your left (you may need to click the 'Refresh' button).\n",
    "\n",
    "<span style=\"color:gray; font-style:italic\">We made this data subset using `time_series_data_preparation.ipynb`. You might find this approach useful when doing your project work.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_filename = \"ls8_lasrc_swiss_fribourg_example.nc\"\n",
    "import os\n",
    "if os.path.exists(nc_filename):\n",
    "    print('File already downloaded.')\n",
    "else:\n",
    "    print('Downloading...')\n",
    "    import requests\n",
    "    URL = \"https://drive.switch.ch/index.php/s/D8mj6rg6VQvlbAw/download\"\n",
    "    response = requests.get(URL)\n",
    "    open(nc_filename, \"wb\").write(response.content)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the prepared Landsat 8 subset for the Fribourg region \n",
    "ds = xr.open_dataset('ls8_lasrc_swiss_fribourg_example.nc', engine='netcdf4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds - the dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:8px solid black\">\n",
    "<a name='part1'></a>\n",
    "\n",
    "# Part 1: Temporal Data\n",
    "## Time components\n",
    "\n",
    "Of special interest for us is the `time` dimension. The different components are explained in more detail also here:https://docs.xarray.dev/en/stable/user-guide/time-series.html#datetime-components.\n",
    "`time` has multiple attributes that will allow you later on to select data of interest. One can first have a look at all the time steps in the dataset by simply calling `<xarrayDataArray>.time`. In the cell below you will see that the time of each scene is stored in a very detailed format:\n",
    "- 2013-04-18T10:18:18.000000000\n",
    "\n",
    "with:\n",
    "- 2013 - year\n",
    "- 04 - month\n",
    "- 18 - day\n",
    "- 10:18:18.000000000 - Hour:Minute:Second\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.time\n",
    "# ds[\"time\"]  # will yield the same output / different way of writing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the individual parts using the same writing but with an additional `.dt` followed by the attribute of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples\n",
    "ds.time.dt.month\n",
    "# ds.time.dt.day\n",
    "# ds.time.dt.year\n",
    "# ds.time.dt.season"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "> NOTE: The date/time string is in a format that we understand (years, months, days, etc.). Inside a computer, the date/time is represented as a numeric value. A standard way is to represent any date as number of days since \"1970-01-01\". This allows to convert the date/time string into something meaningful for the computer.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import dates\n",
    "\n",
    "print(dates.date2num(np.datetime64('1850-11-17 13:12:11')))\n",
    "print(dates.date2num(np.datetime64('1970-01-01 00:00:00')))  # this is the standard time starting point\n",
    "print(dates.date2num(np.datetime64('2022-11-17')))\n",
    "\n",
    "# The output unit is [days since start]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series\n",
    "\n",
    "In this example, along the time axis of the DataArray`ndvi` every pixel (x,y / lon,lat) represents the evolution of the Normalized Difference Vegetation Index (NDVI). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map below shows the average value (`.mean()`) over the time axis (`dim=\"time\"`) for all scenes (images) available in September (`month==9`) for the year 2013 (`year==2013`).\n",
    "\n",
    "This example reduces the dimensions of the DataArray (`ndvi`) in 3D from:\n",
    "- \"time\"\n",
    "- \"latitude\"/y\n",
    "- \"longitude\"/x\n",
    "\n",
    "to 2D:\n",
    "- \"latitude\"/y\n",
    "- \"longitude\"/x\n",
    "\n",
    "You can see this by just calling the objects `ndvi` and `xs` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi = ds.ndvi\n",
    "ndvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = ds.ndvi.sel(time=np.logical_and(ds.ndvi.time.dt.month == 9, ds.ndvi.time.dt.year == 2013)).mean(dim=\"time\")\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.plot.imshow(vmin=0, vmax=1, cmap=cm.BrBG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For time-series analysis, the temporal information of the the `\"time\"` dimension is relevant. Extracting a time series for a pixel is like using a cookie cutter to cut a piece of lasagna through all its layers.\n",
    "Because the 3D DataArray looks like a 3D matrix, one can use indices for the rows and columns. The `xarray` and `geopandas` object types allow also for selecting pixels using the `longitude` and `latitude` information.\n",
    "\n",
    "From the information in the `config_cell.txt` file and from the cell output above, you can see that the spatial extent is:\n",
    "- xmin = 7.15 ºE\n",
    "- xmax = 7.25 ºE\n",
    "- ymin = 46.7 ºN\n",
    "- ymax = 46.8 ºN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a point in the middle of the study area\n",
    "point_coords = [7.195, 46.802]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in which \"dimensions\" is the information stored?\n",
    "ndvi.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the .sel() method you select certain data. You define the dimension (dimension name)\n",
    "# in which the value should be looked for. In the example these are \"longitude\" and \"latitude\"\n",
    "\n",
    "da = ndvi.sel(\n",
    "    latitude=point_coords[0],      # point_coords[0] is the 1st entry (python indices start counting at 0!)\n",
    "    longitude=point_coords[1],     # point_coords[1] is the 2nd entry (python indices start counting at 0!)\n",
    "    method=\"nearest\"               # the nearest method finds the 1 closest pixel\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the output, the dimensions have been reduced. Lon and Lat are only single values and are not dimensions any more\n",
    "da.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.plot.line('o-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows for one pixel each time step as a blue point. The points are connected if consecutive time steps are not separated by a data gap. One can also select more pixels. The following example shows for the same latitude the selection of points along 4 different longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [7.195, 46.802]  \n",
    "x_coords = np.arange(7.195, 7.2, .001)\n",
    "x_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_coord = 46.802              # fixed latitude\n",
    "da = ndvi.sel(  \n",
    "    latitude=y_coord, \n",
    "    longitude=x_coords,\n",
    "    method=\"nearest\"          # the nearest method finds the 1 closest pixel\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview map with positions indicated by circles\n",
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "# First plot the mean NDVI of the whole time series as a map\n",
    "ndvi.mean(dim='time').plot.imshow(vmin=0,\n",
    "                   vmax=1,\n",
    "                   cmap=cm.BrBG)\n",
    "\n",
    "# On top of the map, show the locations of our positions\n",
    "ax.plot(x_coords, \n",
    "     np.repeat(y_coord, len(x_coords)),\n",
    "     marker='o',\n",
    "     linestyle='none',\n",
    "     fillstyle='none',\n",
    "     color='red',\n",
    "     markersize=8)\n",
    "\n",
    "# Draw a box around the positions\n",
    "ax.add_patch(Rectangle((7.192, 46.8), .01, .005,facecolor=\"#FF000022\", edgecolor='r'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the individual time series for the selected 5 points in the map above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.plot.line(x='time', marker='o', markersize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that the 5 locations have some variability. They share many data gaps, which could indicate that these result from cloud cover.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing / Preparing data\n",
    "A single data point (in our case pixel) might show a lot of missing data and might not be fully representative for a landscape unit, like a forest, a lake, a city, or a crop field. One way to account for this is to select ***multiple pixels*** and calculate their mean. The following cells give some examples to calculate a spatial mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- ### loc - Locate/Select (rows and columns)\n",
    "As already shown two cells above, the `.loc[]` method. It is a **method/function** even though it uses **square brackets** - this is to highlight that its purpose is *selecting* parts of the dataset in a similar way as for matrices and other *standard python objects*. `.loc[]` selects rows and columns by its names. In a `xarray.DataArray` the **row and column names (spatial)** are the latitudes (y) and longitudes (x). By default, time takes always the first dimension.\n",
    " -->\n",
    " \n",
    "### `.sel()` - masking data entries of interest\n",
    "\n",
    "Here we use the `.sel()` method. It requires keywords or arguments. For `.sel()` these are the names of the `dimensions`. This makes the method very easy to apply because we *explicitly* define the dimensions on which to perform the selection. In the example these keywords/arguments can be any, some, or all of the dimensions:\n",
    "\n",
    "- 1st dimension: `\"time\"`\n",
    "- 2nd dimension: `\"latitude\"`\n",
    "- 3rd dimension: `\"longitude\"`\n",
    "\n",
    "To tell the method which selection we want to have, we define a **single value** to look for (e.g. `time='2019-10-30'`), or a **range** (`longitude=slice(7.192, 7.193)`). The `slice()` function is interpreted directly by `.sel()` to know that all the values between the first (7.192) and the last value (7.193) should be found.\n",
    "\n",
    "\n",
    "**Examples**\n",
    "\n",
    "Specific dates and date ranges:\n",
    "- `time='2019-10-12'` - one date - will find a time step and its values only if there is data on that day!\n",
    "- `time='2019-10-13'` - this one will not find anything because there is no data on that day.\n",
    "- `time='2019-10-13', method=\"nearest\"` - one date, and method='nearest' because the exact time entry is: `2019-10-12T10:17:17`. This will return the value from the day before.\n",
    "- `time=slice('2019-10-11', '2019-10-13')` - this one will return all the entries in the time `slice`\n",
    "***\n",
    "All dates of the same month:\n",
    "- `time=ndvi.time.dt.month==4` - select all time steps where the month is April\n",
    "- `time=ndvi.time.dt.month.isin([1, 2, 3])` - select all time steps where the month is are either: January, February, or March\n",
    "***\n",
    "Spatial:\n",
    "- `\"latitude\"=46.7\"` - the point at latitude 46.7. If there is no data at 46.7 then you will get an empty DataArray.\n",
    "- `\"latitude\"=46.8, method=\"nearest\"` - the latitude closest to the value 46.8\n",
    "- `\"latitude\"=slice(46.8, 46.81)` - all latitudes between 46.8 and 46.81\n",
    "- `\"longitude\"` - same as for latitude\n",
    "\n",
    "You can combine exact and \"nearest\" selections by using two `.sel()` operations:\n",
    "\n",
    "`ndvi.sel(time='2019-10-13', method='nearest').sel(latitude=slice(46.8, 46.81))`\n",
    "\n",
    "\n",
    "<span class='dothis'>Now try out a different date, with and without the `method='nearest'`, and a `slice(<date-start>, <date-end>)` operation.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = ndvi.sel(time='2019-10-12')\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is the one that was introduced before. It will take **all time steps**, the **latitudes** between 46.805 and 46.8 (reverse order because latitudes increase from bottom to top, but image coordinates increase from top to bottom), and **longitudes** from 7.192 to 7.193. No `method='nearest'` is required here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = ndvi.sel(                                # no time is defined --> all time steps\n",
    "              latitude=slice(46.805, 46.8),   # latitudes from 46.80 to 46.805\n",
    "              # NOTE: the order ^  ,  ^   is the higher and then the lower latitude. That is\n",
    "              #because the image coordinates go from top to bottom, but latitudes \n",
    "              #go from south to north --> botttom to top. That's why they are reversed\n",
    "              longitude=slice(7.192, 7.193))  # longitudes from 7.192 to 7.193\n",
    "da\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "`.sel()` allows also to select certain months, seasons, or years by asking where the **time components** match a condition. In the example below the expression `ndvi.time.dt.month==4` asks where the `month` component matches the value `4` (April). <span class='dothis'>Try to select all time steps from the `ndvi` DataArray that correspond to summer `JJA` (June, July, August) using the **time component**:`.time.dt.season`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the time dimension values are show with the additional \".time\" at the end. For the whole dataset, remove this ending.\n",
    "ndvi.sel(time=ndvi.time.dt.month==4).time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "Often there are many ways that lead to the desired result. There is not a single ***correct way*** to do many things. The following example will extract all the values for the months December and January using three different methods. The results are stored in the variables `a`, `b`, and `c` and plotted to showcase if they are identical.\n",
    "\n",
    "\n",
    ">\n",
    "<font color=blue> The `.sel()` method is the most flexible one - it can be used for almost all selection purposes for time series analysis.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All steps with January or December\n",
    "a = da.loc[np.logical_or(da.time.dt.month==12, da.time.dt.month==1),: ,: ]\n",
    "b = da.where((da[\"time.month\"]==12) | (da[\"time.month\"]==1)) \n",
    "c = da.sel(time=np.logical_or(da.time.dt.month == 12, da.time.dt.month==1))\n",
    "\n",
    "# Showing that the two methods find the same data points\n",
    "a.sel(latitude=7.19,longitude= 46.8,method='nearest').plot.line('o',markersize=15,c='black', label='da.loc[]')\n",
    "b.sel(latitude=7.19,longitude= 46.8,method='nearest').plot.line('o',markersize=10,c='yellow',label='da.where()')\n",
    "c.sel(latitude=7.19,longitude= 46.8,method='nearest').plot.line('o',markersize=5, c='red',   label='da.sel()')\n",
    "plt.legend()\n",
    "\n",
    "alen = len(a.sel(latitude=da.latitude.values[0],longitude= da.longitude.values[0]))\n",
    "blen = len(b.sel(latitude=da.latitude.values[0],longitude= da.longitude.values[0]))\n",
    "clen = len(c.sel(latitude=da.latitude.values[0],longitude= da.longitude.values[0]))\n",
    "print('Length of `time` with .loc[] is :', alen)\n",
    "print('Length of `time` with .where() is :', blen)\n",
    "print('Length of `time` with .sel() is :', clen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.groupby()` - apply operations over dimensions \n",
    "\n",
    "The `groupby` function takes one **dimension** and identifies its unique entries. For those dimensions that have multiple identical values, e.g. **longitude**, it means that all pixels are grouped together with e.g. a longitude of 7.192. \n",
    "After the grouping, an operation has to be applied, e.g. to calculate the `.mean()` of all these values that were grouped. If there are **more than one dimension** over which this operation can be applied, one has to specify over which this shall take place. See the examples below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean values for \"vertical stripes\", i.e. per longitude take all the values of the different latitudes and average them\n",
    "da_grpByLon = da.groupby('longitude').mean('latitude')\n",
    "# Note in the output that the dimension latitude disappeared, and we only have time and longitude left\n",
    "da_grpByLon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_grpByLon.plot.line(x='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can specify also multiple dimensions (axes) over which to apply the function. The next example applies the `.mean()` for each time step (identified with `.groupby()`), by averaging over both spatial dimensions/axes `('longitude', 'latitude')`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean values over the entire \"red box\" that is shown in the map earlier\n",
    "da_grpByLonLat = da.groupby('time').mean(('latitude', 'longitude'))\n",
    "# da_grpByLonLat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting plot shows the average value for each time step in the red box shown in the map.\n",
    "da_grpByLonLat.plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `.resample()`\n",
    "Similar to groupby, `.resample()` will generate a new dataset by e.g. averaging over the `\"time\"` dimension. The main purpose of `.resample()` is to create new temporal resolutions/aggregations:\n",
    "- `D` - daily\n",
    "- `M` - monthly\n",
    "- `Y` - annual\n",
    "- `Q` - quarterly (4 months)\n",
    "- ... and more\n",
    "\n",
    "<span class='dothis'>Resample the `da` DataArray to daily and annual quarterly values.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da_monthly = da.resample(time='M').mean()\n",
    "\n",
    "# Note in the output that the new monthly dates always stop on the last day of the month.\n",
    "# To have the dates be the first day of the month, one can add \"S\" into the resample argument string:\n",
    "\n",
    "da_monthly = da.resample(time='M').mean()\n",
    "\n",
    "da_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da_annual = da.resample(time='AS').mean()\n",
    "# da_annual.sel(longitude=7.193 , latitude=46.8, method='nearest').plot.line()\n",
    "\n",
    "# da_seasonal = da.resample(time='QS').mean()\n",
    "# da_seasonal.sel(longitude=7.193 , latitude=46.8, method='nearest').plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining different methods\n",
    "The methods `.groupby()`, `.resample()`, and `.sel()` will always create a new `xarray.DataArray` as an output. As such, one can again apply the previously mentioned methods.\n",
    "This allows for running multiple processing steps in one line. In the following example, the monthly mean values are calculated **(1)** for all pixels individually, and **(2)** then averaged over the spatial dimension. In the end, there will be one time series (only time dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daa = ndvi.sel(latitude=slice(46.805 , 46.8),\n",
    "               longitude=slice(7.192, 7.193)).groupby('time').mean(('latitude', 'longitude')).resample(time='MS').mean()\n",
    "                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daa.plot.line('-o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top:8px solid black\">\n",
    "<a name='part2'></a>\n",
    "\n",
    "# Part 2: Trends, variability, deviation\n",
    "This section will first introduce the ideas behind these three keywords. Then there will be examples on how to extract them and what pre-processing is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trends\n",
    "\n",
    "In our day-to-day life the words **trend** and **tendency** can often be used interchangeably; in the context of climate, however, they are different. \n",
    "\n",
    "> *Climate describes the average weather conditions for a particular location and over a long period of time.\n",
    "> $[...]$ climate normals—30-year historical averages of variables like temperature and precipitation [...]* (WMO 2022)\n",
    "\n",
    "The main use: **trend** is a **statistically significant change** over time in our variable. With other words, over a time period in a climatological context (>= 30 years), the values increase or decrease; and there is a very small chance that this is observed by chance. If there is less than **30 years** of data available, one can use the term **tendency** (this is not an agreed-upon term!!!) to highlight that this is not a climatological context.\n",
    "\n",
    "However, if we **de-trend** our data, it means that any systematic increase in values over time is removed. This independent of whether this change over time is **significant or non-significant**.\n",
    "\n",
    "***\n",
    "\n",
    "In order to be statistically significant the following must be fulfilled:\n",
    "- $p \\le \\alpha$\n",
    "\n",
    "The significance level $\\alpha$ is chosen by us (usually 5%). In the example of a trend, it is the probability of observing a change over time even though in reality that is not true. *The general definition: it is the probability of rejecting the Null-hypothesis $H_{0}$ (=no trend), given that the Null-hypothesis is true.*\n",
    ">With other words, we allow to make a mistake in 5% of the cases by assuming there is a trend even though there is no trend. Lowering the value of $\\alpha$ makes us more sure there is really a trend, but it also makes it more difficult to find one that is not as obvious.\n",
    "\n",
    "The $p-value$ (sometimes written only $p$) is the result of a statistical test. In the example of a trend, it is the probability of seeing a change over time as extreme as we do, assuming ($H_{0}$) there is no real trend. *The general definition: it is the probability obtaining a result as extreme, given that $H_{0}$ is true.*\n",
    "\n",
    "> Example: $H_{0} =$\"no trend\", $\\alpha=0.05$, $p-value=0.0231$ (outcome of our analysis). Because $p \\le \\alpha$, we reject $H_{0}$ and accept $H_{1}$ the alternative hypothesis that there is a trend. The result is statistically significant at our chosen significance level $\\alpha=0.05$. The lower the $p-value$, the less likely that an identified trend was identified even though in reality there is no trend. Reversing the wording: The lower the $p-value$, the more likely there is a real trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Calculating trends/tendencies \n",
    "There are various packages for the calculation of trends and associated statistics. While `xarray` directly provides a fitting over time function `.polyfit()` it does not provide the statistics like $p-value$ or the goodness of fit. Instead we use the `linregress` function from the `scipy.stats` package.\n",
    "\n",
    "In order to determine the change over time, it is neccessary to represent the time in a continuous format. As Shown in part 1, there are functions that convert the date/time string into a numeric format. In order to convert the date/time string to numeric and vice versa, you have two functions in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.dates import date2num, num2date\n",
    "\n",
    "# two functions to convert back and forth\n",
    "def xr_date2num(time):\n",
    "    return date2num(time)\n",
    "\n",
    "def xr_num2date(time_numeric):\n",
    "    # transforms the num2date (days since ...) into datetime64 (seconds since ...)\n",
    "    return np.array([np.datetime64(d) for d in num2date(time_numeric)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check that the two functions work as intended:\n",
    "\n",
    "# # forward: date to numeric\n",
    "# xr_date2num(daa.time)\n",
    "# # backward:\n",
    "# xr_num2date(xr_date2num(daa.time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `linregress()`\n",
    "`linregress` takes two arrays of values (x and y) to check if there is a relationship between them. `x` will be time and `y` the NDVI values. If there is a significant relationship between them, it means there is a significant change over time; with other words a statistically significant `trend` or `tendency`.\n",
    "\n",
    "The output of `linregress` is multiple statistics. You can check them by uncommenting the `linregress?` line in the next cell. These are in short:\n",
    "\n",
    "- Slope of the regression line.\n",
    "- Intercept of the regression line.\n",
    "- Pearson correlation coefficient. The square of `rvalue` is equal to the coefficient of determination.\n",
    "- p-value for a hypothesis test whose null hypothesis is that the slope is zero, using Wald Test with t-distribution of the test statistic. \n",
    "- and others\n",
    "\n",
    "***\n",
    "#### Slope\n",
    "The **slope** says how much the change is **per time unit**. If we use monthly data, then the change would normally be **change per month**. However, <span style=\"color:red\"> we use the transformed time in days! So we get the slope in units of NDVI/days</span>. \n",
    "\n",
    "#### Intercept\n",
    "The **intercept** is graphically the point on the y-axis where the regression line cuts through it at **x=0**. This statistic is **only of interest for the graphical interpretation** in our case. <span style=\"color:red\"> But keep in mind that time starts \"1970-01-01\"</span>.\n",
    "\n",
    "#### Correlation coefficient (r) and coefficient of determination (r$^{2}$)\n",
    "The **correlation coefficient** and the **coefficient of determination r$^{2}$** tell us how much of the variance is explained. With other words, how well our regression explains the relationship. You will always have a **low $p-value$ if the r$^{2}$ is high**. But: <span style=\"color:red\"> You can have a **low $p-value$ but also a low r$^{2}$**.</span>\n",
    "\n",
    "#### p-value\n",
    "Statistically, maybe the most important outcome. **Is there really a change over time, or do we see something by chance?**. The lower, the more robust/striking.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "# linregress?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non-valid data points `NaN`\n",
    "The `linregress` function is very strict with regards to missing data. We can only use data where there are no missing values (`NaN`). The next cell filters them away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our values for y and x\n",
    "y = daa.values  \n",
    "x = xr_date2num(daa.time.values)\n",
    "\n",
    "# this checks if the value is a valid numeric data point\n",
    "clean_mask = np.isfinite(y)  \n",
    "\n",
    "# the mask has the indices of valid data in y.\n",
    "# you can compare the before and after:\n",
    "\n",
    "# y\n",
    "# y[clean_mask]\n",
    "\n",
    "# The cleaning is applied to both:\n",
    "# - time\n",
    "# - ndvi\n",
    "# so that they have the same length\n",
    "y_clean = y[clean_mask]\n",
    "x_clean = x[clean_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally the regression\n",
    "result = linregress(x_clean, y_clean)\n",
    "print(result)\n",
    "\n",
    "# We are only interested in\n",
    "# - slope \n",
    "# - intercept (only graphical)\n",
    "# - p-value\n",
    "# - r-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "The output shows us that $p > \\alpha$. The change over time is thus not statistically significant.\n",
    "\n",
    "The information on what the optimal (*ordinary least square regression*) regression looks like is stored in the `result` object. The `slope` and `intercept` can be assessed with `result.slope` and `result.intercept`; slope with `result.slope`, $p-value$ with `result.pvalue`, and $r$ with `result.rvalue`.\n",
    "\n",
    "The line can be created by plotting the x-values (time) against the values calculated from the simple formula for a line:\n",
    "\n",
    "$y = m*x + b$,\n",
    "\n",
    "where $m$ is the slope, $b$ is the intercept, $x$ are the time values (in days), and $y$ are the NDVI values.\n",
    "***\n",
    "\n",
    "The following is an example of a non-significant trend:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daa.plot.line()\n",
    "# add the regression line\n",
    "m = result.slope\n",
    "b = result.intercept\n",
    "\n",
    "# back-transform the time? Not needed, because matplotlib knows!\n",
    "y_pred = m * x + b     # the predicted values; using all time steps (non-filtered)\n",
    "plt.plot(x, y_pred, 'bo-', markersize=3)\n",
    "\n",
    "# But you can try. Comment out the command before, and use the following two lines instead:\n",
    "# x_rev = xr_num2date(x)\n",
    "# plt.plot(x_rev, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope units\n",
    "As mentioned before, we transform the time. If we look at the `slope` value, we can see a value of `-4.828947187444238e-06`. Since the transformed time values have the units of **days**, this value indicates a change of **-4.828947187444238e-06 per day**.\n",
    "\n",
    "We can check quickly by looking at the predicted values and corresponding time entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = x[0]\n",
    "t1 = x[1]\n",
    "\n",
    "dt = t1 - t0\n",
    "print('The difference in days between the two time steps:',dt)\n",
    "\n",
    "# the predicted NDVI values\n",
    "ndvi_predicted = m * x + b\n",
    "ndvi_pred0 = ndvi_predicted[0]\n",
    "ndvi_pred1 = ndvi_predicted[1]\n",
    "\n",
    "dndvi = ndvi_pred1 - ndvi_pred0\n",
    "print('The difference in NDVI from the regression between the two time steps:',dndvi)\n",
    "\n",
    "rate = dndvi/dt\n",
    "print('The slope is:',rate)\n",
    "\n",
    "# Quick check if this rate is the same as from the regression (ratio=1):\n",
    "print(\"This should be close to 1:\",rate/result.slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Complete examples\n",
    "\n",
    "In the following you have two examples with a full workflow:\n",
    "- selecting data\n",
    "- averaging the data\n",
    "- resampling to monthly time steps\n",
    "- filtering NaN values\n",
    "- running linregress\n",
    "- plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monthly example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial subset\n",
    "da = ndvi.sel(latitude=slice(46.8, 46.79),  # higher value before lower for latitudes!!\n",
    "              longitude=slice(7.178, 7.19))\n",
    "# select all values of a certain month\n",
    "da_mon = da.sel(time=da.time.dt.month == 9)\n",
    "# resample to monthly means\n",
    "da_mon = da_mon.resample(time='MS').mean()\n",
    "# average over all latitudes and longitudes per time step\n",
    "da_mon = da_mon.groupby('time').mean(('longitude','latitude'))\n",
    "\n",
    "# regression preparation:\n",
    "y = da_mon.values  \n",
    "x = xr_date2num(da_mon.time.values)  \n",
    "\n",
    "# only take non-NaN values\n",
    "clean_mask = np.isfinite(y)  \n",
    "\n",
    "y_clean = y[clean_mask]\n",
    "x_clean = x[clean_mask]\n",
    "\n",
    "# regression\n",
    "reg = linregress(x_clean, y_clean)\n",
    "\n",
    "# print results\n",
    "print(reg)\n",
    "\n",
    "# slope\n",
    "m = reg.slope\n",
    "# intercept\n",
    "b = reg.intercept\n",
    "\n",
    "# calculate regression line (all months)\n",
    "y_pred = m * x + b\n",
    "# calculate regression line (only September)\n",
    "y_pred_mon = m * x_clean + b\n",
    "\n",
    "# plot\n",
    "da_mon.plot.line('bo-')\n",
    "plt.plot(x_clean, y_pred_mon, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annual example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial subset\n",
    "da = ndvi.sel(latitude=slice(46.8, 46.79),  # higher value before lower for latitudes!!\n",
    "              longitude=slice(7.178, 7.19))\n",
    "# select all values of a certain year\n",
    "da_mon = da.sel(time=da.time.dt.year == 2014)\n",
    "# resample to monthly means\n",
    "da_mon = da_mon.resample(time='MS').mean()\n",
    "# average over all latitudes and longitudes per time step\n",
    "da_mon = da_mon.groupby('time').mean(('longitude','latitude'))\n",
    "\n",
    "# regression preparation:\n",
    "y = da_mon.values  \n",
    "x = xr_date2num(da_mon.time.values)  \n",
    "\n",
    "# only take non-NaN values\n",
    "clean_mask = np.isfinite(y)  \n",
    "\n",
    "y_clean = y[clean_mask]\n",
    "x_clean = x[clean_mask]\n",
    "\n",
    "# regression\n",
    "reg = linregress(x_clean, y_clean)\n",
    "\n",
    "# print results\n",
    "print(reg)\n",
    "\n",
    "# slope\n",
    "m = reg.slope\n",
    "# intercept\n",
    "b = reg.intercept\n",
    "\n",
    "# calculate regression line (all months)\n",
    "y_pred = m * x + b\n",
    "# calculate regression line (only September)\n",
    "y_pred_mon = m * x_clean + b\n",
    "\n",
    "# plot\n",
    "da_mon.plot.line('bo-')\n",
    "plt.plot(x_clean, y_pred_mon, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Spatio-temporal trends\n",
    "It is possible to calculate the trend through time for every pixel in our datacube. This allows to compare pixel by pixel in the form of a map.\n",
    "\n",
    "To do this, `xarray` provides a special function: `apply_ufunc()`. The output is very helpful in understanding in more detail, where we observe statistically significant trends (Is it at fields, in the city, ...?). And we can use these results also to select these areas for further analysis (not covered here).\n",
    "\n",
    "The example below shows first the function and then the output in the next cell for the `slope`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import linregress\n",
    "\n",
    "dataset = ndvi\n",
    "\n",
    "# Don't change anything in the code below.\n",
    "x = np.arange(dataset.time.shape[0])\n",
    "\n",
    "def new_linregress(y):\n",
    "    # Wrapper around scipy linregress to use in apply_ufunc\n",
    "    clean_mask = np.isfinite(y)  \n",
    "    y_clean = y[clean_mask]\n",
    "    x_clean = x[clean_mask]\n",
    "    slope, intercept, r_value, p_value, _ = linregress(x_clean, y_clean)\n",
    "    return np.array([slope, intercept, r_value, p_value])\n",
    "\n",
    "stats = xr.apply_ufunc(new_linregress, dataset, \n",
    "                       input_core_dims=[['time']],\n",
    "                       output_core_dims=[[\"parameter\"]],\n",
    "                       vectorize=True,\n",
    "                       dask=\"parallelized\",\n",
    "                       output_dtypes=['float64'],\n",
    "                       output_sizes={\"parameter\": 4},\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output is a bit different here. We have two spatial dimensions (longitude and latitude), and an array 'parameter'\n",
    "stats.dims\n",
    "\n",
    "# Inside the 'paramter' array we have the 4 columns\n",
    "# slope, intercept, r_value, p_value \n",
    "# this is the result of the \"return np.array([slope, intercept, r_value, p_value])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the easiest way to access the outputs is by using the positional index\n",
    "# Column 1 - slope (index 0)\n",
    "# Column 2 - intercept (index 1)\n",
    "# Column 3 - rvalue (index 2)\n",
    "# Column 4 - pvalue (index 3)\n",
    "\n",
    "ndvi_slope = stats[:,:,0]  # slope\n",
    "ndvi_slope.name = 'slope'  # change the name from 'ndvi' to 'slope'\n",
    "ndvi_slope.plot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class='dothis'>1) Change the `parameter` from slope to $p-value$.</span>\n",
    "\n",
    "<span class='dothis'>2) Change the dataset from `da` to the full dataset `ndvi`.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a name='part3'></a>\n",
    "# Part 3: Variability\n",
    "\n",
    "Variability refers to how much a variable like NDVI changes in general, as compared to how much the values change systematically over time (--> trends/tendencies). The monthly example from before shows no statistically significant trend ($p > \\alpha$). But we see that the values change a lot. Some example for high variability can be different crops on the fields that result in differentt NDVI values, different precipitation patterns in combination with temperature that lead to variable snow cover, etc.\n",
    "\n",
    "A common statistic to describe variability is the **standard deviation**. \n",
    "\n",
    "\n",
    "$s = \\sqrt\\frac{\\sum{(x_i-\\bar{x})^2}}{n}$\n",
    "\n",
    "The standard deviation has the same unit as the data in the time series. It makes it therefore more intuitive to use it instead of the ***variance***.\n",
    "\n",
    "\n",
    "Another useful way to investigate variability is by looking at the **deviation from the mean**, sometimes called anomalies. Instead of calculating a single statistic over all time steps, one derives for each time step a value.\n",
    "\n",
    "\n",
    "### Application\n",
    "#### Standard deviation\n",
    "\n",
    "One can directly calculate the standard deviation for each pixel by calling the function `.std('time')`, indicating that it should be applied over the **time** dimension.\n",
    "\n",
    "The following example shows directly the difference between the urban and the rural area in terms of NDVI variability. Crops fields can easily be identified where the variability is especially high.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi.std('time').plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same example but only for the month of August\n",
    "ndvi.sel(time=ndvi.time.dt.month==8).std('time').plot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "#### Deviation from the mean\n",
    "As the name says, we have to calculate the mean first and subtract this value from each individual NDVI value. If there is a strong seasonality, we have to think of which mean we calculate (monthly, annual, ...), and of which data we subtract this mean (also monthly, annual, ...).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_annual_mean = ndvi.mean('time')\n",
    "da_annual = ndvi.resample(time='AS').mean()\n",
    "da_dev_from_mean = da_annual - da_annual_mean\n",
    "\n",
    "# plot the time series for a pixel:\n",
    "da_dev_from_mean_pixel = da_dev_from_mean.sel(longitude=7.19, latitude=46.793, method='nearest')\n",
    "\n",
    "da_dev_from_mean_pixel.plot.line('ko-')\n",
    "plt.hlines(y= da_dev_from_mean_pixel.mean(), \n",
    "           xmin=da_dev_from_mean_pixel.time[0], \n",
    "           xmax=da_dev_from_mean_pixel.time[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the deviation from the mean for the year 2018 - as a map\n",
    "# da_dev_from_mean.sel(time=da_dev_from_mean.time.dt.year==2018)[0].plot.imshow()\n",
    "da_dev_from_mean.sel(time=da_dev_from_mean.time.dt.year==2018).mean(dim='time').plot.imshow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the easiest way to access the outputs is by using the positional index\n",
    "# this line from the cell above defines the order:\n",
    "# return np.array([slope, intercept, r_value, p_value])\n",
    "# Column 1 - slope (index 0)\n",
    "# Column 2 - intercept (index 1)\n",
    "# Column 3 - rvalue (index 2)\n",
    "# Column 4 - pvalue (index 3)\n",
    "ndvi_slope = stats[:,:,0]  # pvalue\n",
    "ndvi_slope.name = 'slope'\n",
    "ndvi_slope.plot.imshow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Open Data Cube)",
   "language": "python",
   "name": "odc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
